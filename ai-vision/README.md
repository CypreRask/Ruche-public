# Vision par Intelligence Artificielle

DÃ©tection et comptage d'abeilles vs frelons asiatiques en temps rÃ©el avec YOLOv11n.

## ğŸ¯ Mon travail

J'ai rÃ©alisÃ© l'ensemble du pipeline ML : collecte de donnÃ©es, entraÃ®nement, optimisation edge et dÃ©ploiement.

## ğŸ“Š Dataset

**Bee-Hornet-Detect v1** - [Roboflow Universe](https://universe.roboflow.com/imgprocess-n3bpn/bee-hornet-detect)

- **Images** : 2,238 images annotÃ©es
- **Classes** : 
  - `0` : Frelon asiatique (Vespa velutina)
  - `1` : Abeille (Apis mellifera)
- **Format** : YOLOv11 (auto-orientation + resize 640x640 stretch)
- **Licence** : CC BY 4.0
- **Preprocessing** : Roboflow (auto-orientation, resize 640x640)

## ğŸ‘¤ CrÃ©dits

*   **Mathis** : EntraÃ®nement ModÃ¨les, Pipeline MLOps, Optimisation Edge.
*   **Didier Orlandi** : Enseignant FabLab, Mentor Technique & Initiateur du projet.

## ğŸ§  MÃ©thodologie d'EntraÃ®nement

Le modÃ¨le **YOLOv11n** (architecture nano) a Ã©tÃ© choisi pour son ratio performance/coÃ»t computationnel optimal pour l'edge computing.

### Protocole
- **Transfer Learning** : Fine-tuning Ã  partir des poids prÃ©-entraÃ®nÃ©s sur COCO.
- **HyperparamÃ¨tres** :
  - `imgsz`: 640x640 (standard)
  - `epochs`: 100 (convergence observÃ©e Ã  ~85 epochs)
  - `batch`: 48 (saturant GPU T4)
  - `optimizer`: AdamW (meilleure convergence sur petits datasets)
- **Data Augmentation** :
  - **Mosaic** : ActivÃ© pour amÃ©liorer la dÃ©tection de petits objets et la robustesse aux occlusions (contexte dense de ruche).
  - **Mixup** : DÃ©sactivÃ© (contre-productif sur classes morphologiquement proches).

**RÃ©sultat** : `models/best.pt` (~5.4 MB).

## ğŸ“ˆ Performance & MÃ©triques

Voici les rÃ©sultats de l'entraÃ®nement final sur le dataset de 2238 images :

| Courbe F1 | Courbe PR |
|:---:|:---:|
| ![F1 Curve](runs/detect/bee_yolo11n_robotflow/F1_curve.png) | ![PR Curve](runs/detect/bee_yolo11n_robotflow/PR_curve.png) |
| **F1-Score optimal** : 0.87 | **mAP@0.5** : 0.89 |

### Matrice de Confusion & Labels
![Labels](runs/detect/bee_yolo11n_robotflow/labels.jpg)

## âš¡ StratÃ©gie de DÃ©ploiement & Quantification

L'enjeu critique Ã©tait le dÃ©ploiement sur **Raspberry Pi 4 (CPU ARM Cortex-A72)**. Une Ã©tude comparative des frameworks d'infÃ©rence a Ã©tÃ© menÃ©e.

### Comparatif d'InfÃ©rence (Benchmark)

| Framework | Format | PrÃ©cision | Latence Moy. | FPS (Pi 4) | Observation |
|-----------|--------|-----------|--------------|------------|-------------|
| **PyTorch** | FP32 | Baseline | ~1200 ms | ~0.8 | Inutilisable en temps rÃ©el. |
| **TFLite** | INT8 | Quantized | ~240 ms | ~4.1 | Perte de mAP significative due Ã  la calibration statique. |
| **NCNN** | FP16 | Optimized | ~190 ms | **~5.2** | **Meilleur trade-off.** Optimisation native pour instructions ARM NEON. |

> **Choix final : NCNN**
> Le framework NCNN a Ã©tÃ© retenu pour sa gestion efficace de la mÃ©moire et ses kernels optimisÃ©s pour ARMv8, permettant d'atteindre >5 FPS, seuil minimal pour le tracking d'insectes.

Script d'export et quantification : [`export/export_edge.py`](export/export_edge.py)

## ğŸ”§ Modes de fonctionnement

Le dÃ©tecteur propose 2 modes (voir [`core/ruche_detector.py`](core/ruche_detector.py)) :

### Mode Production
- **Usage** : Comptage continu, faible consommation CPU
- **Config** : `imgsz=320`, `stride=10`, `max_det=10`
- **Perf** : ~1-2 FPS effectif

### Mode DÃ©mo
- **Usage** : PrÃ©sentations, fluiditÃ© visuelle
- **Config** : `imgsz=640`, `stride=2`, `max_det=30`
- **Perf** : ~4-5 FPS

## ğŸ“ˆ Benchmarks

J'ai rÃ©alisÃ© une grid-search complÃ¨te pour optimiser les hyperparamÃ¨tres sur Raspberry Pi :

### Comparaison des formats
Script : [`benchmark/bench_compare.py`](benchmark/bench_compare.py)

Compare PyTorch vs NCNN vs TFLite sur le mÃªme jeu de vidÃ©os.

### Grid-search optimisÃ©
Script : [`benchmark/bench_optimized.py`](benchmark/bench_optimized.py)

Teste diffÃ©rentes configurations :
- RÃ©solutions : 320, 256, 224
- Strides : 2, 3, 4
- Objectif : Atteindre 5+ FPS

### Tests limites
Script : [`benchmark/bench_limits.py`](benchmark/bench_limits.py)

Configurations ultra-rapides pour voir les limites :
- 192px, 160px, 128px
- Stride agressif

## ğŸš€ Utilisation

### Test rapide sur vidÃ©o
```bash
cd benchmark
python test_video.py chemin/vers/video.mp4
```

### Serveur de streaming
```bash
cd core
python video_server.py
```

Le serveur dÃ©marre sur le port 2002 avec :
- Streaming MJPEG (`/video_feed`)
- Changement de source dynamique (`/set_source`)
- Scan des camÃ©ras (`/scan_devices`)

### EntraÃ®nement d'un nouveau modÃ¨le
```bash
cd training
python train.py
```

*Note : NÃ©cessite le dataset Roboflow tÃ©lÃ©chargÃ© localement.*

## ğŸ“ Structure

```
ai-vision/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best.pt                 # ModÃ¨le entraÃ®nÃ© (YOLOv11n)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ ruche_detector.py       # Wrapper YOLO (2 modes)
â”‚   â””â”€â”€ video_server.py         # Serveur FastAPI + MJPEG
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train.py                # Script d'entraÃ®nement
â”œâ”€â”€ export/
â”‚   â””â”€â”€ export_edge.py          # Export NCNN/TFLite
â””â”€â”€ benchmark/
    â”œâ”€â”€ bench_compare.py        # Comparaison formats
    â”œâ”€â”€ bench_optimized.py      # Grid-search
    â”œâ”€â”€ bench_limits.py         # Tests ultra-rapides
    â””â”€â”€ test_video.py           # Test simple
```

## ğŸ”— DÃ©pendances

```bash
pip install ultralytics opencv-python torch
```

Pour TFLite :
```bash
pip install tflite-runtime  # ou tensorflow
```

Pour NCNN (optionnel) :
```bash
pip install ncnn
```

## ğŸ”¬ Perspectives & Limitations

### 1. Tracking Multi-Objets (MOT)
L'implÃ©mentation actuelle utilise une dÃ©tection frame-by-frame. L'intÃ©gration d'un tracker lÃ©ger type **ByteTrack** permettrait de :
- RÃ©duire le comptage multiple d'un mÃªme insecte (ID unique).
- Analyser les trajectoires (comportement d'attaque du frelon vs vol stationnaire).

### 2. Quantification AvancÃ©e (QAT)
L'export actuel utilise du Post-Training Quantization (PTQ). Un **Quantization-Aware Training (QAT)** permettrait de rÃ©cupÃ©rer les ~2% de mAP perdus lors du passage en INT8, rendant l'option TFLite/Coral plus viable.

### 3. AccÃ©lÃ©ration Hardware
Le passage sur un NPU dÃ©diÃ© (ex: **Hailo-8L** sur Pi 5 ou **Coral USB**) permettrait de monter Ã  30+ FPS, dÃ©bloquant l'analyse comportementale fine.
